\documentclass[11pt, a4paper]{article}
\usepackage[a4paper]{geometry}
\usepackage{listings}
\usepackage[usenames, dvipsnames]{color}
\usepackage{titlesec}


\titleformat{\section}
{\color{DarkOrchid}\normalfont\LARGE\bfseries}
{\color{DarkOrchid}\thesection}{1em}{}

\titleformat{\subsection}
{\color{DarkOrchid}\normalfont\Large\bfseries}
{\color{DarkOrchid}\thesubsection}{1em}{}

\titleformat{\subsubsection}
{\color{DarkOrchid}\normalfont\bfseries}
{\color{DarkOrchid}\thesubsubsection}{1em}{}

\definecolor{SoftGray}{RGB}{244, 244,244}

\lstset{ 
  language=C++,               
  basicstyle = \footnotesize,
  backgroundcolor = \color{SoftGray},
  frame=single,                   
  breaklines=true,                
}


\begin{document}
\title{\bf\color{DarkOrchid}BAM!\\
\bf"Simplify your Parallelism"
 }
\author{Stephan Dollberg}
\date{\today}
\maketitle

\section{parallel constructs}
bam::parallel constructs offer simplistic parallel replacements for standard library algorithms like \textsc{std::for\_each}.
\\\\All Parallel-Algorithms share a typical interface. They all take a range, a worker function and a grainsize parameter. 
Work is split into pieces and worked on by a limited count of threads which is determined on runtime. Task stealing is performed when a thread runs out of work. When no grainsize parameter is passed, the default value is 0, which means that implementation will choose a grainsize on runtime.

\subsection{parallel\_for}

The interface looks like this:

\begin{lstlisting} 
template<typename ra_iter, typename worker_predicate>
void parallel_for(ra_iter begin, ra_iter end, worker_predicate worker, int grainsize = 0)
\end{lstlisting}
\texttt{bam::parallel\_for} is a template which is the parallel replacement for a basic for-loop. Its use is pretty simple and straight forward, the following example should make it quite clear.

\subsubsection{Example 1: }

\begin{lstlisting} 
  typedef std::vector<int> container;
  typedef container::iterator iter;

  container v = {1, 2, 3, 4, 5, 6};

  auto worker = [] (iter begin, iter end) {
    for(auto it = begin; it != end; ++it) {
      *it = compute(*it);
    }
  };

  bam::parallel_for(std::begin(v), std::end(v), worker);
\end{lstlisting}
The snippet shows how to parallelize a simple compute operation which modifies each element in a vector. All that is needed is a worker lambda which takes a range then performs \texttt{compute} on every element in that range. 

\subsection{parallel\_for\_each}
\begin{lstlisting}
template<typename ra_iter, typename worker_predicate>
void parallel_for_each(ra_iter begin, ra_iter end, worker_predicate worker, int grainsize = 0)
\end{lstlisting}

\texttt{bam::parallel\_for\_each} is the replacement for \texttt{std::for\_each}. It functions the very same as the original does and basically all you have to do is to replace the name. The following example shows a simple use case.

\subsubsection{Example 1: Compute}
\begin{lstlisting}
  typedef std::vector<int> container;
  typedef container::iterator iter;

  container v = {1, 2, 3, 4, 5, 6};

  auto worker = [] (int& i) { i = compute(i); };
  bam::parallel_for_each(std::begin(v), std::end(v), worker);
\end{lstlisting}

Taking up the \texttt{bam::parallel\_for} example and making it yet even easier, we basically just wrote a typically easy \texttt{std::for\_each} function predicate and all we had to do was to replace the namespace identifier. 

\subsection{parallel\_reduce}
\texttt{bam::parallel\_reduce} is doing simple data parallel tasks but unlike bam::parallel\_for it does reduce operations and returns a value.
\\\\The interface is definied as followed:
\begin{lstlisting}
template<
typename return_type, 
typename ra_iter, 
typename predicate, 
typename join_predicate, 
>
return_type parallel_reduce(ra_iter begin, ra_iter end, predicate worker, join_predicate join_worker, int grainsize = 0)

\end{lstlisting}

The interface is quite big but mighty, the following examples will explain different use-cases and the several arguments.

\begin{center}
  \begin{tabular}{ | l |  p{10cm} | }
    \hline
	begin,  end & begin and end form the range on which \texttt{bam::parallel\_reduce} works on \\ \hline
	worker & worker is the predicate which takes a range and does the work on it \\  \hline
	join\_worker & join predicate which joins the results of the two binary splitted parts \\ \hline
	grainsize & sets the grainsize parameter for the minimal range of splitting \\ \hline
  \end{tabular}
\end{center}

\subsubsection{Example 1: Accumulate}

\begin{lstlisting}
  typedef std::vector<int> container;
  typedef container::iterator iter;

  container v = {1, 2, 3, 4, 5, 6};

  using namespace std::placeholders;
  std::function<int(iter, iter)> accumulate_wrapper = 
     std::bind(std::accumulate<iter, int>, _1, _2, 0);

  int result = bam::parallel_reduce<int>(std::begin(v), std::end(v), accumulate_wrapper, std::plus<int>());

  std::cout << result << std::endl; // prints 21
\end{lstlisting}

This example shows the very basics when using \texttt{bam::parallel\_reduce}. At first we are creating a vector whose elements we want to accumulate in parallel. Then we define a wrapper, which calls \texttt{std::accumulate} as we can't just pass \texttt{std::accumulate} to our algorithm as that one can't provide the third parameter to \texttt{std::accumulate} which is needed for type deduction. In our call to \texttt{bam::parallel\_reduce} we do explicitly name the return type as, that can't be deduced from the template arguments. We then pass our range in form of the iterators, our function to work on, the accumulate wrapper, the \texttt{std::plus<int>} functor which is the needed join function to combine the splitted parts which the different threads are handling. 

\subsubsection{Example 2: Find Max}
\begin{lstlisting}
  typedef std::vector<int> container;
  typedef container::iterator iter;

  container v = {1, 2, 3, 4, 5, 6};

  auto join_max_helper = [] (iter a, iter b) -> iter {
      return *a > *b ? a : b;
  };

  iter result = bam::parallel_reduce<iter>(std::begin(v), std::end(v), std::max_element<iter>, join_max_helper);

  std::cout << *result << std::endl; // prints 6
\end{lstlisting}

In this example we want to find the biggest element in a given range. Therefore we use the new C++11 standard function \texttt{std::max\_element} as a worker function, however this time we have to provide a custom join function which returns the iterator with the biggest associated element of the two given iterators.

\section{async}
\subsection{async}
\texttt{bam::async} is a replacement for \texttt{std::async}. It fixes some of the mistakes made in \texttt{std::async}, which will probably be fixed in forthcoming standards. 

\subsubsection{ Non-Blocking \texttt{std::future} on \texttt{std::launch::async} invocation }

\begin{lstlisting}
  std::atomic<bool> var(true);
  auto deadlocker = [&] { while(var) { std::this_thread::yield(); }}
  bam::async(std::launch::async, deadlocker);
  var = false;
\end{lstlisting}

The above example would deadlock if one used \texttt{std::async} instead of \texttt{bam::async}. 

\subsubsection{ Backed by a task\_pool on all implementations}

If you launch \texttt{bam::async} with either \texttt{std::launch::async | std::launch::deferred} or with none launch policy specified the task will be run asynchronously or deferred backed by a task\_pool. This is a fix for implementations which don't offer this by default(e.g. gcc). 

You might ask what the "or" means? This just means that the implementation will start the task asynchronously as long there is a free thread in internal managing scheduler, if there are no threads available, the task will be started as \texttt{std::launch::deferred}. This behavior is needed to make recursive invocations of \texttt{bam::async} not deadlock. Using \texttt{std::launch::async} creates a "fire and forget" task which you shouldn't use recursively as that might deadlock the implementation.

\subsubsection{ Different launch behaviors} 

\begin{lstlisting}
  auto worker = [] { std::cout << "hi from async" << std::endl;};
  // will launch asynchronously in task_pool
  bam::async(std::launch::async, worker); 
  // will launch asynchronously or deferred
  bam::async(worker);
  // same as above
  bam::async(std::launch::async | std::launch::deferred, worker);
  // same as std::async(std::launch::deferred, worker)
  bam::async(std::launch::deferred, worker); 


\end{lstlisting}

\section{Task Pool}
\subsection{ Basic Task Pool}

The Task Pool class is - in the classical sense - a threadpool. You can add tasks which will be added to the internal queue of the taskpool. It will decide on runtime how many threads will run and work on the tasks. \\\\
The class offers the following interface:\\

\begin{lstlisting}

  task_pool();

  template<typename function, typename ...Args>
  std::future<typename std::result_of<function(Args...)>::type> add(function&& f, Args&& ...args);

  void wait();

\end{lstlisting}

The following examples will make the use pretty clear:

\subsubsection{Example 1: Run a simple task}

\begin{lstlisting}
  bam::task_pool pool;

  pool.add([] () { std::cout << "Hello from a task pool!" << std::endl; });

\end{lstlisting}

We simply create a \texttt{task\_pool} to which we add a function which prints a string to the output. 

\subsubsection{Example 2: Run a task with arguments}
\begin{lstlisting}
  bam::task_pool pool;

  pool.add([] (int x) { std::cout << "The argument was " << x << std::endl;}, 42);
\end{lstlisting}
This time we run a function which takes an argument. The interface for \texttt{bam::task\_pool::add} is basically the standard interface for functions which take a predicate and arguments for that. 

\subsubsection{Example 3: Wait for the pool to finish}
\begin{lstlisting}
  bam::task_pool pool;
  for(auto i = 0; i != 100; ++i) {
    pool.add(compute);
  }

  pool.wait();
\end{lstlisting}
The example shows how to wait for a group of tasks if you need to have it finished before passing on in your program. The \texttt{bam::task\_pool::wait} function waits for all tasks to finish and after that puts the pool back into a waiting state.

\subsubsection{Example 4: Getting the return value of a function passed to a taskpool}
\begin{lstlisting}
  bam::task_pool pool;

  std::future<int> ret = pool.add([] () { return 42; });

  // do some stuff

  std::cout << ret.get() << std::endl;

\end{lstlisting}
The example shows how one can get the return value of a function which is passed to the task\_pool. \texttt{bam::task\_pool::add} returns a \texttt{std::future<>} which you can use to obtain the value returned or exception thrown by the function. 

\subsubsection{Example 5: Misusing a task\_pool}
This example shows how you should not use a \texttt{task\_pool}.

\begin{lstlisting}
  auto worker = [] (int& x) { x *= compute(x); };
  std::vector<int> v2 = init_vector();

  bam::task_pool pool;
  for(auto& i : v2) {
    pool.add(worker, std::ref(i));
  }
  pool.wait_and_finish();
\end{lstlisting}
This example is a perfect usecase for the \texttt{parallel\_for\_each} template. There is too much overhead involved when using a \texttt{task\_pool}. The \texttt{task\_pool} first starts to pay out when the functions get more complex and the problem can't be packed into an embarrassingly parallel solution.

\section{Timer}
\subsection{Timer}

The Timer class 
\begin{lstlisting}
template<typename resolution>
class timer;
\end{lstlisting}

 is a template class which wraps some \texttt{std::chrono} stuff into an easy to use class. The template type referes to a \texttt{std::chrono} duration type. The following examples  will make it's use pretty clear.
\subsubsection{Example 1: Measure a single event with basic\_timer}
\begin{lstlisting}
  bam::basic_timer t;
  compute();
  std::cout << t.elapsed() << std::endl;
\end{lstlisting}

This is the most simple usecase, at first we create a timer object. On creation, the timer starts automatically, we then compute something and finally print the elapsed time since the timer started. \texttt{basic\_timer} is a typedef for \texttt{timer<std::chrono::milliseconds>}.
\subsubsection{Example 2: Measure a series of events}
The next example will show another feature which makes the timer class perfect for timing and comparing different functions.

\begin{lstlisting}
  bam::timer<std::chrono::seconds> t;
  compute1();
  std::cout << t.since_last_epoch() << std::endl;
  compute2();
  std::cout << t.since_last_epoch() << std::endl;
  compute3();
  std::cout << t.since_last_epoch() << std::endl;
\end{lstlisting}

This time we changed our resolution type to \texttt{std::chrono::seconds}. We call 3 different compute functions and print how much time each function needed. \texttt{timer::since\_last\_epoch} returns the time which elapsed since the last call to either \texttt{timer::elapsed} or \texttt{timer::since\_last\_epoch}, in contrary to \texttt{timer::elapsed} which always returns the time since the creation of the timer.


\section{parallel\_invoke}
\subsection{parallel\_invoke}

\texttt{parallel\_invoke} enables you to invoke n-number of functions in parallel. It's basically a simple wrapper around \texttt{std::async}, the disadvantage is though that you can't save the futures from each single function.

\subsubsection{Example 1: Invoking 3 functions in parallel}

\begin{lstlisting}
  bam::parallel_invoke([] { std::cout << "hello from 1" << std::endl; }, [] { std::cout << "hello from 2" << std::endl; }, [] { std::cout << "hello from 3" << std::endl; });
\end{lstlisting}

If any of the given functions throws an exception, the exception will be rethrown.

\subsubsection{Example 2: Throwing an exception}

\begin{lstlisting}
  bam::parallel_invoke([] { throw std::runtime_error("hello from exception")});
\end{lstlisting}

\end{document}